We noted in our earlier assignment that the use of GAE helps make advantage calculations smoother by managing bias and variance with λ=0.95, and we chose mini-batching to handle 
data in batches of 64 over 5 epochs, which improved learning steadiness as we saw a drop at the end of the graph in past tries, while also cutting down memory use during updates. 
The switch to updated learning rates of 0.0001 for policy and 0.001 for value also cut down on overfitting compared to earlier rates of 0.0005 for both, which led to overfitting without 
extra stabilizing methods. On top of that, we added entropy regularization with a coefficient of 0.06 to encourage exploration and avoid quick settling, and used gradient clipping 
with a max norm of 0.3 to keep updates steady, further lowering variation and boosting training reliability.

General Advantage Estimation (GAE):
Reason: GAE reduces variance in advantage estimates compared to raw Monte-Carlo returns while controlling bias. It uses a parameter  λ=0.95 to balance between Monte-Carlo returns (high variance, low bias) 
and single-step TD errors (low variance, high bias). In the code, GAE computes advantages as improving the stability of policy updates.
Impact: This helps PPO achieve high rewards (near 500) by providing more reliable advantage estimates, though the drops indicate remaining instability.


Mini-Batching:
Reason: Mini-batching splits the episode data into smaller batches (batch size 64) for gradient updates, processed over 5 epochs. This reduces memory usage and allows more frequent updates within an episode, leading to better gradient estimates and faster convergence.
Impact: It contributes to the steady rise to 400 by 200K steps, but the variance during updates may contribute to the observed dips, as small batches can introduce noise in gradient directions.

Entropy:
Reason:Without exploration, the agent might converge prematurely to a suboptimal policy, especially in environments like CartPole-v1 where local optima exist. Entropy ensures the policy remains 
stochastic, helping the agent discover better actions and avoid getting stuck.This is particularly useful in early training or when the policy is near convergence, preventing exploitation of 
a limited set of actions.
Impact:The value entropy_coef=0.06 (a moderate setting) balances exploration and exploitation, tailored to the updated learning rates (lr_policy=0.0001, lr_value=0.001) and other tricks

Gradient Clipping:
Reason: Gradient clipping is used to prevent large gradient updates that can destabilize training, especially in deep reinforcement learning where exploding gradients may occur due to 
high variance in rewards or policy updates.
Impact: Gradient clipping ensures stable learning by limiting gradient norms, reducing the risk of divergence and maintaining consistent performance.

Tricks used to generate graph:
The plotting code uses interpolation to align rewards across runs at common steps, averaging them with standard deviation shading to visualize variance. 
It applies a smoothing technique with an exponential moving average (factor=0.9) to reduce noise in the mean reward curve, enhancing readability and trend clarity.

------------------------------------------------------------------------------GRAPH OBSERVATION---------------------------------------------------------------------------------------------------------
COMPARISON BTW PPO VS PPO WITH TRICKS
The learning curves look at standard PPO and PPO with Tricks on CartPole-v1 across 1M steps. GAE (λ=0.95) helps make advantage estimates smoother, pushing both curves up to around 
500 by 600K steps, but PPO with Tricks shows better steadiness, holding near 450 with less ups and downs by 1M steps, while standard PPO falls to 300 with more variation. 
Mini-batching, using 64 batches and 5 epochs, makes learning more stable in both, though PPO with Tricks benefits more, skipping the late drop that standard PPO has. 
Gradient clipping (max_grad_norm=0.3) in PPO with Tricks keeps updates in check, lowering variation and stopping the fall to 300, unlike standard PPO which gets shaky. 
Entropy regularization (entropy_coef=0.06) in PPO with Tricks pushes for more exploration, helping it climb to 500 more smoothly and stay steady, while standard 
PPO has bigger swings, showing how these tricks together make things more reliable and steady.






